{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the correct base URL for the USPTO Patent Grants API\n",
    "base_url = 'https://api.uspto.gov/api/v1/patent/applications/search'\n",
    "\n",
    "# Define the query parameters to filter patents by grant date (2009)\n",
    "params = {  # Filter patents granted in 2009\n",
    "    'start': 0,  # Start at record 0 (for pagination)\n",
    "    'rows': 100  # Number of patents to return per request (adjustable)\n",
    "}\n",
    "\n",
    "# Make the request to the USPTO API\n",
    "response = requests.get(base_url, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    patents = response.json()['results']\n",
    "    # Loop through the returned patents and print basic information\n",
    "    for patent in patents:\n",
    "        print(f\"Patent Number: {patent['patentApplicationNumber']}\")\n",
    "        print(f\"Title: {patent['inventionTitle']}\")\n",
    "        print(f\"Date: {patent.get('publicationDate')}\")\n",
    "        print(\"-\" * 80)\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}, {response.text}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Define the correct base URL for the USPTO Patent Grants API\n",
    "base_url = 'https://developer.uspto.gov/ibd-api/v1/application/publications'\n",
    "\n",
    "# Initialize a set to keep track of downloaded patent application numbers\n",
    "downloaded_patents = set()\n",
    "\n",
    "# Function to make a query and update the downloaded_patents set\n",
    "def fetch_patents(start, rows=100):\n",
    "    params = {\n",
    "        'start': start,\n",
    "        'rows': rows\n",
    "    }\n",
    "    response = requests.get(base_url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        patents = response.json()['results']\n",
    "        new_patents = [patent for patent in patents if patent['patentApplicationNumber'] not in downloaded_patents]\n",
    "        for patent in new_patents:\n",
    "            downloaded_patents.add(patent['patentApplicationNumber'])\n",
    "        return new_patents\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}, {response.text}\")\n",
    "        return []\n",
    "\n",
    "# Loop to fetch multiple batches of patents\n",
    "all_patents = []\n",
    "start = 0\n",
    "while True:\n",
    "    new_patents = fetch_patents(start)\n",
    "    if not new_patents:\n",
    "        break\n",
    "    all_patents.extend(new_patents)\n",
    "    start += 100\n",
    "\n",
    "# Print the number of unique patents downloaded\n",
    "print(f\"Total unique patents downloaded: {len(all_patents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert the patents list into a pandas DataFrame\n",
    "patents_df = pd.DataFrame(all_patents)\n",
    "\n",
    "# Flatten the lists in 'abstractText', 'claimText', and 'descriptionText' columns\n",
    "for col in ['abstractText', 'claimText', 'descriptionText']:\n",
    "    patents_df[col] = patents_df[col].apply(lambda x: x[0] if isinstance(x, list) and len(x) == 1 else x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patents_df.to_pickle('all_patents.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start from here to download data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patents_df = pd.read_pickle('all_patents.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the DataFrame\n",
    "patents_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patents_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(patents_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patents_df['descriptionText'][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, desc in enumerate(patents_df['descriptionText'][:5]):\n",
    "    desc = '\\n'.join([desc[i:i+100] for i in range(0, len(desc), 100)])\n",
    "    print(\"Application Number: \" + patents_df['patentApplicationNumber'][i])\n",
    "    print(\"Invention Title: \" + patents_df['inventionTitle'][i])  \n",
    "    print(f\"{'-'*80}\")\n",
    "    print(f\"Description {i+1}:\\n{desc}\\n{'-'*80}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec requires sentences as input\n",
    "from nltk import sent_tokenize\n",
    "from string import punctuation\n",
    "translator = str.maketrans('','',punctuation) \n",
    "from nltk.corpus import stopwords\n",
    "stoplist = set(stopwords.words('english'))\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def normalize_text(doc):\n",
    "    \"Input doc and return clean list of tokens\"\n",
    "    doc = doc.replace('\\r', ' ').replace('\\n', ' ')\n",
    "    lower = doc.lower() # all lower case\n",
    "    nopunc = lower.translate(translator) # remove punctuation\n",
    "    words = nopunc.split() # split into tokens\n",
    "    nostop = [w for w in words if w not in stoplist] # remove stopwords\n",
    "    no_numbers = [w if not w.isdigit() else '#' for w in nostop] # normalize numbers\n",
    "    stemmed = [stemmer.stem(w) for w in no_numbers] # stem each word\n",
    "    return stemmed\n",
    "\n",
    "def get_sentences(doc):\n",
    "    sent=[]\n",
    "    for raw in sent_tokenize(doc):\n",
    "        raw2 = normalize_text(raw)\n",
    "        sent.append(raw2)\n",
    "    return sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = []\n",
    "for doc in patents_df['descriptionText'][0:5000]:\n",
    "    sentences += get_sentences(doc)\n",
    "from random import shuffle\n",
    "\n",
    "shuffle(sentences) # stream in sentences in random order\n",
    "\n",
    "\n",
    "\n",
    "# Create progress bar callback\n",
    "class ProgressBar(gensim.models.callbacks.CallbackAny2Vec):\n",
    "    def __init__(self, total_epochs):\n",
    "        self.tqdm = None\n",
    "        self.epoch = 0\n",
    "        self.total_epochs = total_epochs\n",
    "\n",
    "    def on_train_begin(self, model):\n",
    "        self.tqdm = tqdm(total=self.total_epochs, desc=\"Training Progress\")\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        self.epoch += 1\n",
    "        self.tqdm.update(1)\n",
    "        if self.epoch >= self.total_epochs:\n",
    "            self.tqdm.close()\n",
    "\n",
    "\n",
    "# Define total number of epochs for progress tracking\n",
    "total_epochs = 10\n",
    "\n",
    "# Create a callback instance\n",
    "progress_bar = ProgressBar(total_epochs=total_epochs)\n",
    "\n",
    "# train the model\n",
    "\n",
    "w2v = Word2Vec(sentences,  # list of tokenized sentences\n",
    "               workers = 8, # Number of threads to run in parallel\n",
    "               vector_size=300,  # Word vector dimensionality     \n",
    "               min_count =  25, # Minimum word count  \n",
    "               window = 5, # Context window size      \n",
    "               sample = 1e-3, # Downsample setting for frequent words\n",
    "               callbacks=[progress_bar] # Add the callback to the model\n",
    "               )\n",
    "\n",
    "# done training, so delete context vectors\n",
    "w2v.init_sims(replace=True)\n",
    "\n",
    "w2v.save('w2v-vectors.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = list(w2v.wv.key_to_index.keys())\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector format of word 'invent'\n",
    "w2v.wv['invent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Length of the vector\n",
    "len(w2v.wv['invent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.most_similar('invent') # most similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "info = api.info()  # show info about available models/datasets\n",
    "model = api.load(\"glove-wiki-gigaword-300\")  # download the model and return as object ready for use\n",
    "model.most_similar(\"invent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import fasttext.util\n",
    "# fasttext.util.download_model('en', if_exists='ignore')  # English\n",
    "ft = fasttext.load_model('cc.en.300.bin')\n",
    "\n",
    "# Example usage\n",
    "# print(ft.get_word_vector(\"king\"))      # 300-dim vector\n",
    "# print(ft.get_nearest_neighbors(\"queen\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    return dot(v1, v2) / (norm(v1) * norm(v2))\n",
    "\n",
    "\n",
    "# Word2Vec\n",
    "print(\"Word2Vec similarity:\", w2v.wv.similarity('machin', 'devic')) # similarity between two words\n",
    "print(\"GloVe similarity:\", model.similarity('machine', 'device'))\n",
    "print(\"FastText similarity:\", cosine_similarity(ft.get_word_vector('machine'), ft.get_word_vector('device')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec\n",
    "print(\"Word2Vec similarity:\", w2v.wv.similarity('may', 'june')) # similarity between two words\n",
    "print(\"GloVe similarity:\", model.similarity('may', 'june'))\n",
    "print(\"FastText similarity:\", cosine_similarity(ft.get_word_vector('may'), ft.get_word_vector('june')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec: K-Means Clusters\n",
    "from sklearn.cluster import KMeans\n",
    "kmw = KMeans(n_clusters=50)\n",
    "kmw.fit(w2v.wv.vectors)\n",
    "\n",
    "invent_clust = kmw.labels_[w2v.wv.key_to_index['invent']]\n",
    "for i, cluster in enumerate(kmw.labels_):\n",
    "    if cluster == invent_clust and i<=100:\n",
    "        print(w2v.wv.index_to_key[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list docs so that each element is a list of sentences of each document in descriptionText\n",
    "docs = []\n",
    "for doc in patents_df['descriptionText'][:1000]:\n",
    "    sentences = get_sentences(doc)\n",
    "    # flatten list of sentences into one list\n",
    "    docs.append([item for sublist in sentences for item in sublist])\n",
    "\n",
    "# flatten the list of sentences into one list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Make document vectors by averaging word embeddings in a document\n",
    "##\n",
    "import numpy as np\n",
    "\n",
    "sentvecs = []\n",
    "for sentence in docs:\n",
    "    vecs = [w2v.wv[w] for w in sentence if w in w2v.wv]\n",
    "    if len(vecs)== 0:\n",
    "        sentvecs.append(np.nan)\n",
    "        continue\n",
    "    sentvec = np.mean(vecs,axis=0)\n",
    "    sentvecs.append(sentvec.reshape(1,-1))\n",
    "# First 30 elements of the first sentence:\n",
    "sentvecs[0][0][:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(sentvecs[0],\n",
    "                  sentvecs[1])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the pairwise cosine similarity between all sentences:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out NaN values from sentvecs\n",
    "filtered_sentvecs = [vec for vec in sentvecs if not isinstance(vec, float)]\n",
    "\n",
    "# Stack the sentence vectors into a single numpy array\n",
    "sentvecs_array = np.vstack(filtered_sentvecs)\n",
    "\n",
    "# Calculate the pairwise cosine similarity\n",
    "similarity_matrix = cosine_similarity(sentvecs_array)\n",
    "\n",
    "# Display the first 5x5 block of the similarity matrix\n",
    "similarity_matrix[:5, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most similar sentences that are not on the diagonal\n",
    "max_similarity = -1\n",
    "most_similar_sentences = None\n",
    "for i in range(similarity_matrix.shape[0]):\n",
    "    for j in range(similarity_matrix.shape[1]):\n",
    "        if i != j and similarity_matrix[i, j] > max_similarity:\n",
    "            max_similarity = similarity_matrix[i, j]\n",
    "            most_similar_sentences = (i, j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask the diagonal by setting it to a very low value\n",
    "np.fill_diagonal(similarity_matrix, -np.inf)\n",
    "\n",
    "# Find the indices of the maximum value in the similarity matrix\n",
    "i, j = np.unravel_index(np.argmax(similarity_matrix), similarity_matrix.shape)\n",
    "\n",
    "# The most similar sentences\n",
    "most_similar_sentences = (i, j)\n",
    "max_similarity = similarity_matrix[i, j]\n",
    "\n",
    "print(f\"Most similar sentences are at indices: {most_similar_sentences} with similarity score: {max_similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the indices of the most similar sentences\n",
    "j = most_similar_sentences[0]\n",
    "\n",
    "# Display the titles and abstracts of the most similar sentences\n",
    "print(\"Title 1:\", patents_df.iloc[0]['inventionTitle'])\n",
    "print(\"Abstract 1:\", patents_df.iloc[0]['abstractText'])\n",
    "print(\"\\nTitle 2:\", patents_df.iloc[j]['inventionTitle'])\n",
    "print(\"Abstract 2:\", patents_df.iloc[j]['abstractText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the titles and abstracts of the most similar sentences\n",
    "print(\"Title 1:\", patents_df.iloc[934]['inventionTitle'])\n",
    "print(\"Abstract 1:\", patents_df.iloc[934]['abstractText'])\n",
    "print(\"\\nTitle 2:\", patents_df.iloc[992]['inventionTitle'])\n",
    "print(\"Abstract 2:\", patents_df.iloc[992]['abstractText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
