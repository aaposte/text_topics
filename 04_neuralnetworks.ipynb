{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11463396",
   "metadata": {},
   "source": [
    "This notebook is inspired by Chapter 1 of the book **\"Natural Language Processing with Transformers: Building Language Applications with Hugging Face\"** by Tunstall, von Werra, and Wolf.\n",
    "\n",
    "You will get first experience in using pretrained models for specific tasks, using the *pipeline* API from the Hugging Face Transformes library, which allows you to do inference at a very high level of abstraction.\n",
    "\n",
    "In each exercise, you will first define a *pipeline* for a specific task, using a pretrained model from the Hugging Face models library, and apply the pipeline on text snippets from the course webpage.\n",
    "\n",
    "## Resources:\n",
    "\n",
    "- [pipeline documentation](https://huggingface.co/docs/transformers/main_classes/pipelines)\n",
    "- [Hugging Face models library](https://huggingface.co/models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111f4383",
   "metadata": {},
   "source": [
    "## The input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf97376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "\n",
    "text = \"\"\"Last year has shown multiple breakthroughs in deep learning, bringing large language\n",
    "models to the mainstream. OpenAI's ChatGPT, Microsoft's new Bing Search and GitHub\n",
    "Copilot, and Deep Mind's AlphaCode are the most prominent. While they still have many\n",
    "flaws, they show a potential to transform many sectors of the economy, replace some\n",
    "workers and make other vastly more productive.\n",
    "\n",
    "NLP also has an immense potential to change research in economics. Most economists use\n",
    "small and expensive structured datasets. NLP offers a way to work with novel data sources\n",
    "that often can be scraped for free from the web. Examples are classifying speeches along\n",
    "the political spectrum, classifying tweets to measure opinions, extracting concepts\n",
    "mentioned in free-form survey replies, or translating questionnaires or datasets into\n",
    "different languages.\n",
    "\n",
    "This class is an introduction to deep learning and NLP for economists. Starting from\n",
    "zero, the first half of the course focuses on learning the practical skills needed to\n",
    "incorporate NLP into empirical workflows. We will use Huggingface's transformers library\n",
    "and only work with pre-trained models for this. The second half of the class zooms in\n",
    "and focuses on understanding what language models are, how they differ, and how they are\n",
    "trained. We will write some purely didactical code in numpy and implement a few simple\n",
    "models in PyTorch. The main focus of the second half is to build enough understanding to\n",
    "work effectively with pre-trained models. It is beyond our scope and computational\n",
    "resources to actually train large models.\"\"\"\n",
    "\n",
    "paragraphs = text.split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e818eff",
   "metadata": {},
   "source": [
    "## Text classification\n",
    "\n",
    "1. Define a classifier pipeline using the ` \"distilbert-base-uncased-finetuned-sst-2-english\"` model.\n",
    "\n",
    "2. Apply the pipeline to the entire text.\n",
    "\n",
    "3. Apply the pipeline separately on each paragraph of the input text to extract sentiments.\n",
    "\n",
    "4. Convert the output of the previous task to a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fade688",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\n",
    "    \"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eff9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac590994",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = classifier(paragraphs)\n",
    "sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e10be7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier(\"awful terrible bad horrible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55e9a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92d77c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_emotion = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"bhadresh-savani/distilbert-base-uncased-emotion\",\n",
    "    return_all_scores=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bce17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_emotion(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34470028",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = classifier_emotion(paragraphs)\n",
    "emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e3ccf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_emotion(\"neutral\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184ff92a",
   "metadata": {},
   "source": [
    "## Named entity recognition\n",
    "1. Define a named entity recognition (ner) pipeline using the `\"dslim/bert-base-NER-uncased\"` model. \n",
    "\n",
    "2. Apply the pipeline to `text` and convert the result to a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16813824",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_tagger = pipeline(\n",
    "    \"ner\",\n",
    "    model=\"dslim/bert-base-NER-uncased\",\n",
    ")\n",
    "pd.DataFrame(ner_tagger(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835aa833",
   "metadata": {},
   "source": [
    "## Aggregation strategies\n",
    "\n",
    "Use the same model as before, but try out different aggregation strategies. Which one gives you the best results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e14f40",
   "metadata": {},
   "source": [
    "**Note**: Below we show a solution where you apply all strategies in a loop and combine the result in one DataFrame. Other solutions are completely ok. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43b10bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_results = []\n",
    "for strategy in [\"none\", \"simple\", \"first\", \"average\", \"max\"]:\n",
    "    tagger = pipeline(\n",
    "        \"ner\",\n",
    "        model=\"dslim/bert-base-NER-uncased\",\n",
    "        aggregation_strategy=strategy,\n",
    "    )\n",
    "    df = pd.DataFrame(tagger(text))\n",
    "    df[\"strategy\"] = strategy\n",
    "    ner_results.append(df)\n",
    "\n",
    "pd.concat(ner_results).set_index(\"strategy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469e63cc",
   "metadata": {},
   "source": [
    "## Question answering\n",
    "\n",
    "1. Define a question answering pipeline using the `\"deepset/roberta-base-squad2\"` model.\n",
    "\n",
    "2. Come up with a few questions one might ask about the course logistics. \n",
    "\n",
    "3. Apply the pipeline to get answers to your questions.\n",
    "\n",
    "**Do not trust any answer without double-checking it!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fa16ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistics_text = \"\"\"Text Data in Economics\n",
    " \n",
    "The goal of this course is to equip you with modern text data methods and integrate these techniques into your research. You should know what type of text methods are used in economic research and have some hands-on experience with text data methods. By the end of the course, you should have an actionable research plan.\n",
    "The course begins with introductory lectures on text data methods and showcases examples of how these methods have been used in economics research. The topics covered include tokenisation, distance in text, vectorisation, and the use of large language models.\n",
    "Following the introductory lectures, we start working on our own text data project and developing our own research ideas using text data. We work individually or in groups depending on the number of students. First, we develop research questions and consider potential data sources that would allow us to answer the question. We explore available data sources and design the analysis we would run on our data. We present the ideas to the group, write a research proposal, and give feedback to our peers. In the end, we write a term paper detailing the research question, contributions to the literature, data sources, analysis, and an empirical part using text data. If data is too difficult to acquire during the course, students can do a separate text data exercise. Ideally, the plan leads to a proper research project.\n",
    "The course is for economics students in all fields interested in text methods and you will be encouraged to work on a project in your field of interest. However, most examples in the lectures will focus on my fields of expertise like political economy and economic development.\n",
    "Prerequisites:\n",
    "Some Python programming experience is required (or excellent skills in another programming language and a willingness to acquire the necessary skills very fast).\n",
    "There are also prerequisite courses set by the administration: Mathematics for Economists and Basic module Econometrics. I cannot give you credit if you have not done the prerequisite courses!\n",
    "\n",
    "Requirements:\n",
    "You develop and present a research idea to the group.\n",
    "You read your peers’ proposals and provide short, written feedback with suggestions on how to improve them.\n",
    "You engage in the group discussion after each presentation.\n",
    "At the end of the course, you hand in a term paper on the final version of your research proposal.\n",
    " \n",
    "Grading: The final grade is a weighted average of your presentation and participation in the group discussions (40%) and your term paper (60%).  \n",
    " \n",
    " \n",
    " \n",
    " \n",
    "Syllabus\n",
    "\n",
    "Lecture 1: Introduction to Text Data, Scraping and Tokenization\n",
    "Reading: \n",
    "Ash and Hansen, “Text Algorithms in Economics”\n",
    "Gentzkow, Kelly, and Taddy, “Text as Data”\n",
    " \n",
    "Lecture 2: Tokenization and Dictionaries\n",
    "          \tReading: \n",
    " \n",
    "Lecture 3: Vectorization and Document Distance\n",
    "            Reading: \n",
    "Autor et al. “New Frontiers: The Origins and Content of New Work, 1940–2018”\n",
    "\n",
    " \n",
    "Lecture 4: Large Language Models\n",
    " \tReading: \n",
    "\t\t\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "Schedule\n",
    "Presentations: Text Data: Presentations \n",
    "\n",
    "Date\n",
    "Content\n",
    "08.10.\n",
    "Lecture 1: Overview\n",
    "15.10.\n",
    "Lecture 2: Tokenization Dictionaries\n",
    "22.10.\n",
    "Lecture 3: Vectorization and Document Distance\n",
    "29.10.\n",
    "Lecture 4: Language Models\n",
    "05.11.\n",
    "Paper Presentations\n",
    "12.11.\n",
    "Paper Presentations\n",
    "19.11.\n",
    "Paper Presentations\n",
    "26.11.\n",
    "No Lecture (option for Q&A)\n",
    "03.12.\n",
    "Paper Presentations\n",
    "10.12.\n",
    "Paper Presentations\n",
    "17.12.\n",
    "Q&A for Research Proposals (On Zoom)\n",
    "07.01.\n",
    "Research Proposal Presentations\n",
    "14.01.\n",
    "Research Proposal Presentations\n",
    "21.01.\n",
    "Research Proposal Presentations\n",
    "28.02.\n",
    "Term Paper Deadline\n",
    "\n",
    " \n",
    "Notebooks\n",
    "Data for the Notebooks: Link (Dropbox)\n",
    "Notebook1: Corpora Matching Link (Dropbox)\n",
    "Notebook2: Tokenization Link (Dropbox\n",
    "Notebook3: Word Embeddings and Document Distance Link (Dropbox)\n",
    "\n",
    "\n",
    "\n",
    "Instructions for Paper Presentations:\n",
    "Presentations should be 30 minutes with a focus on the text methods in the paper. The presentation should cover the following content:\n",
    "- What is the authors’ motivation and research question? Which gap in the literature do they\n",
    "address (short)?\n",
    "- Which data do they use, how do they preprocess data? How did the research access data, Is it available for anyone?\n",
    "- What text methods did they use?\n",
    "- Are these methods complemented with other empirical methods or a research design to allow studying causal questions?\n",
    "- What are the most important results?\n",
    "- What are the limits of the study? Where do you see potential for future research?\n",
    "- 2-3 questions or thoughts for discussion\n",
    "\n",
    "Instructions for Research Proposal Presentations:\n",
    "Presentations should be 20 minutes with a focus on the text methods in the paper. The presentation should cover the following content:\n",
    "Research Question\n",
    "Existing Literature\n",
    "What data is/will be used\n",
    "What text methods are being used?\n",
    "Is there a theoretical framework, other empirical methods, research design?\n",
    "Some preliminary results?\n",
    "Instructions for the Term Paper:\n",
    "The proposal has to include empirical research using text data.\n",
    "\n",
    "The following content should be included:\n",
    "- Introduction, including the motivation and research question\n",
    "- Overview of the related literature\n",
    "- Suitable data source, how is it accessed?\n",
    "- Description of text data method that will be used\n",
    "- Some analysis on the text*\n",
    "\n",
    "- Optional: discussion of other methods used in the paper\n",
    "Length: maximum 10 pages. Also code and results should be submitted.\n",
    "\n",
    " *I want every student to get some hands-on experience on analyzing text. However, if the data is not accessible fast enough for this course, you should not let that restrict you writing the Term Paper on that idea. So, in this case you can do some analysis on different text data, write a short report and submit that analysis as part of your Term Paper.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed27bfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
    "\n",
    "questions = [\n",
    "    \"What is the deadline for the term paper?\",\n",
    "    \"How long is the research proposal presentation?\",\n",
    "    \"How is the grade determined?\",\n",
    "    \"What is the topic of lecture 3?\",\n",
    "    \"What is the maximum length of the term paper?\",\n",
    "]\n",
    "answers = pd.DataFrame(reader(question=questions, context=logistics_text))\n",
    "answers[\"question\"] = questions\n",
    "\n",
    "answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc7ae8c",
   "metadata": {},
   "source": [
    "## Summarization\n",
    "1. Define a text summarizing pipeline using the `\"sshleifer/distilbart-cnn-6-6\"` model. \n",
    "2. Apply the pipeline to `text`\n",
    "3. Play around with the keyword arguments `min_length` and `max_length` until you get something you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164eccc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-6-6\")\n",
    "\n",
    "summarizer(\n",
    "    text,\n",
    "    min_length=100,\n",
    "    max_length=200,\n",
    "    clean_up_tokenization_spaces=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15120f4",
   "metadata": {},
   "source": [
    "## Summarization by paragraph\n",
    "\n",
    "1. Apply the pipeline from the previous task to each paragraph of `text`.\n",
    "2. Play around with `min_length` and `max_length` until you are satisfied with the result.\n",
    "3. Combine the results back into one string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef9dda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = summarizer(\n",
    "    paragraphs, min_length=40, max_length=60, clean_up_tokenization_spaces=True\n",
    ")\n",
    "texts = [entry[\"summary_text\"] for entry in summaries]\n",
    "print(\"\\n\\n\".join(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00213e63",
   "metadata": {},
   "source": [
    "## Translation\n",
    "\n",
    "1. Go to [huggingface](https://huggingface.co/models) and search for a model that can translate a text from english to your favorite language.\n",
    "2. Define a pipeline to do the translation\n",
    "3. Apply the pipeline on the example input text to translate the content to German."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3158e72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# translator = pipeline(\"translation_en_to_de\", model=\"Helsinki-NLP/opus-mt-en-es\")\n",
    "# See the how-to guide for further instructions\n",
    "\n",
    "# outputs = translator(text, clean_up_tokenization_spaces=True)\n",
    "# outputs[0][\"translation_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e269cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "# Set the API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec391b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define which model to use throughout\n",
    "MODEL = \"gpt-5-nano\"\n",
    "MAX_TOKENS = 8000\n",
    "WAIT_TIME = 0.8  # Wait time between each request. This depends on the rate limit of the model used: GPT-4 needs longer wait time than GPT-3.5.\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f5cc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4.1-nano\",  # Which model to use\n",
    "    temperature=0.2,  # How random is the answer\n",
    "    max_tokens=120,  # How long can the reply be\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is the meaning of life?\"}],\n",
    ")\n",
    "result = \"\"\"\"\"\"\n",
    "for choice in response.choices:\n",
    "    result += choice.message.content\n",
    "print(f\"Model answer: '{result}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562dcb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data from textfiles\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define the folder path where the text files are located\n",
    "folder_path = \"./global-populism-dataset/speeches_20220427/\"\n",
    "\n",
    "# Use glob to get a list of all *.txt files in the folder\n",
    "txt_files = glob.glob(folder_path + \"/*.txt\")\n",
    "\n",
    "# Create an empty list to store the data\n",
    "data = []\n",
    "\n",
    "# Loop through each text file\n",
    "for file_path in txt_files:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as file:\n",
    "        # Read the text from the file\n",
    "        text = file.read()\n",
    "\n",
    "        # Get the filename without the directory path\n",
    "        filename = os.path.basename(file_path)\n",
    "\n",
    "        # Append the text and filename to the data list\n",
    "        data.append({\"filename\": filename, \"text\": text})\n",
    "\n",
    "# Create a dataframe with the data\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f334e085",
   "metadata": {},
   "source": [
    "## 3. Loading and preparing your data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b248f69",
   "metadata": {},
   "source": [
    "The next step is to load and prepare the data that we want to analyze. We will load the data into a Pandas dataframe to allow easy processing.\n",
    "\n",
    "The details of how to open your particular data depends on the structure and format of the data. Pandas offers ways of opening a range of file formats, including CSV and Excel files. You may wish to refer to the Pandas documentation for more details.\n",
    "\n",
    "In our example, we will use the data from the Global Populism Dataset (https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/LFTQEZ). This data offers a number of texts from politicians, and can be used for validating our method. The texts are provided as .txt files in a folder. We will load all these files into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f69c21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data from textfiles\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define the folder path where the text files are located\n",
    "folder_path = \"global-populism-dataset/speeches_20220427/\"\n",
    "\n",
    "# Use glob to get a list of all *.txt files in the folder\n",
    "txt_files = glob.glob(folder_path + \"/*.txt\")\n",
    "\n",
    "# Create an empty list to store the data\n",
    "data = []\n",
    "\n",
    "# Loop through each text file\n",
    "for file_path in txt_files:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as file:\n",
    "        # Read the text from the file\n",
    "        text = file.read()\n",
    "\n",
    "        # Get the filename without the directory path\n",
    "        filename = os.path.basename(file_path)\n",
    "\n",
    "        # Append the text and filename to the data list\n",
    "        data.append({\"filename\": filename, \"text\": text})\n",
    "\n",
    "# Create a dataframe with the data\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed6f030",
   "metadata": {},
   "source": [
    "### Filter the data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc52950d",
   "metadata": {},
   "source": [
    "You will likely need to filter out and select the data you wish to include. \n",
    "\n",
    "In our case, we will filter out texts with non-latin alphabets. While ChatGPT can handle languages with non-latin characters, it is currently more expensive, and there are issues with managing text length. For simplicity, we therefore remove the texts with non-latin alphabet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4854070e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_latin_alphabet(text):\n",
    "    latin_characters = 0\n",
    "    total_characters = 0\n",
    "\n",
    "    for char in text:\n",
    "        if ord(char) >= 0x0000 and ord(char) <= 0x007F:\n",
    "            latin_characters += 1\n",
    "        total_characters += 1\n",
    "\n",
    "    # Check if the majority of characters are Latin alphabet characters\n",
    "    if latin_characters / total_characters >= 0.9:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "df = df[df[\"text\"].apply(is_latin_alphabet)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e9ef19",
   "metadata": {},
   "source": [
    "### Chunking the texts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34537aa",
   "metadata": {},
   "source": [
    "Unlike other NLP methods, not much preprocessing is needed. However, LLMs are only able to process texts that are smaller than their \"context window\". If our texts are longer than the context window of our model, we have to either split the texts into several smaller chunks and analyze them part by part, or simply truncate the text (not recommended).\n",
    "\n",
    "The details depend on the model you use and the amount for data. For our example, with the GPT-4-32k model, our speeches all fit in the model window, and we do not need to split the texts. \n",
    "\n",
    "However, for pedagogical reasons, we will use the standard 8K GPT-4 model and chunk the text into smaller pieces. If your text is short, such as a tweet, this function will do nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66eb7894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to chunk the text into pieces, separated on sentence level.\n",
    "# To do so, we use the nltk library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88378b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tiktoken\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818d5ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import nltk.data\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e727a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code chunks the text into processable pieces of similar size.\n",
    "# If the text is longer than allowed in terms of model tokens, we want to split the text into equally sized parts, without splitting any text mid-sentence.\n",
    "\n",
    "\n",
    "def split_text_into_chunks(text, max_tokens):\n",
    "\n",
    "    # Code the text in gpt coding and calculate the number of tokens\n",
    "    encoding = tiktoken.encoding_for_model(MODEL)\n",
    "    nrtokens = len(encoding.encode(text))\n",
    "\n",
    "    if nrtokens < max_tokens:\n",
    "        return [text]\n",
    "\n",
    "    # how many chunks to split it into?\n",
    "    num_chunks = np.ceil(nrtokens / max_tokens)\n",
    "\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Calculate the number of words per chunk\n",
    "    words_per_chunk = len(text.split()) // num_chunks\n",
    "\n",
    "    # Initialize variables\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    word_counter = 0\n",
    "    # Iterate through each sentence\n",
    "    for sentence in sentences:\n",
    "        # Add the sentence to the current chunk\n",
    "        current_chunk.append(sentence)\n",
    "        word_counter += len(sentence.split())\n",
    "\n",
    "        # Check if the current chunk has reached the desired number of words\n",
    "        if word_counter >= words_per_chunk:\n",
    "            # Add the current chunk to the list of chunks\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            word_counter = 0\n",
    "            # Reset the current chunk\n",
    "            current_chunk = []\n",
    "\n",
    "    # Add the remaining sentences as the last chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3815429b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum number of words per chunk, this depends on the model context window.\n",
    "# We set it to a bit lower than the max tokens, to leave space for our instruction and the response.\n",
    "max_tokens = MAX_TOKENS - 2000\n",
    "df[\"text_chunks\"] = df[\"text\"].apply(lambda x: split_text_into_chunks(x, max_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5579b5",
   "metadata": {},
   "source": [
    "# 4. Prompt engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e99c61",
   "metadata": {},
   "source": [
    "The next step is to formulate a first instructions for analyzing the text. The prompts will be a result of an iterative process through which you develop a formulation of the concept that you wish to capture. \n",
    "\n",
    "We here start by drawing on the instructions for human coders from a previous study.\n",
    "\n",
    "See the how-to guide for details on this process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c942c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"\"\"Your task is to evaluate the level of populism in a political text. Populism is defined as \"an ideology that considers society to be ultimately separated into two homogeneous and antagonistic groups, 'the pure people' versus 'the corrupt elite', and which argues that politics should be an expression of the volonté générale (general will) of the people.\"\n",
    "A populist text is characterized by BOTH of the following elements:\n",
    "- People-centrism: how much does the text focus on \"the people\" or \"ordinary people\" as an indivisible or homogeneous community? Does the text promote a politics as the popular will of \"the people\"?\n",
    "Appeals to specific subgroups of the population (such as ethnicities, regional groups, classes) are inherently antithetical to populism.\n",
    "- Anti-elitism: how much does the text focus on \"the elite\", and to what extent are elites in general described in negative terms? In populist texts, the elite is often described as corrupt, and the juxtaposition between the ordinary people and the elite is cast as a moral struggle between good and bad. \n",
    "Criticism of specific elements within an elite is not populist: a populist appeal must regard the elite in its entirety as anathema. \n",
    "\n",
    "You should give the text a numeric grade between 0 and 2.\n",
    "2. The text is very populist and comes very close to the ideal populist discourse.\n",
    "1. A speech in this category includes strong expressions of all of the populist elements,  but either does not use them consistently or tempers them by including non-populist elements. The text may have a romanticized notion of the people and the idea of a unified popular will, but it avoids bellicose language or any particular enemy.\n",
    "0. A speech in this category uses few if any populist elements. \n",
    "[Answer with a number in the 0-2 range, followed by a semi-colon, and then a brief motivation. For instance: \"1.23; The text shows many elements of a populist text.\" Do not use quotation marks.]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa31460",
   "metadata": {},
   "source": [
    "# 5. Calling the LLM and analyzing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e36381",
   "metadata": {},
   "source": [
    "### 5.1 Call the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95af4e20",
   "metadata": {},
   "source": [
    "We will now write simple functions for calling the API and carry out our analysis request. We will also need to handle possible errors returned from the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff35b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def analyze_message(text, instruction, model=\"gpt-4\", temperature=0.2):\n",
    "    print(f\"Analyzing message...\")\n",
    "\n",
    "    response = None\n",
    "    tries = 0\n",
    "    failed = True\n",
    "\n",
    "    while failed:\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                temperature=temperature,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": f\"'{instruction}'\",\n",
    "                    },  # The system instruction tells the bot how it is supposed to behave\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"'{text}'\",\n",
    "                    },  # This provides the text to be analyzed.\n",
    "                ],\n",
    "            )\n",
    "            failed = False\n",
    "\n",
    "        # Handle errors.\n",
    "        # If the API gets an error, perhaps because it is overwhelmed, we wait 10 seconds and then we try again.\n",
    "        # We do this 10 times, and then we give up.\n",
    "        except openai.APIError as e:\n",
    "            print(f\"OpenAI API returned an API Error: {e}\")\n",
    "\n",
    "            if tries < 10:\n",
    "                print(\n",
    "                    f\"Caught an APIError: {e}. Waiting 10 seconds and then trying again...\"\n",
    "                )\n",
    "                failed = True\n",
    "                tries += 1\n",
    "                time.sleep(10)\n",
    "            else:\n",
    "                print(f\"Caught an APIError: {e}. Too many exceptions. Giving up.\")\n",
    "                raise e\n",
    "\n",
    "        except openai.ServiceUnavailableError as e:\n",
    "            print(f\"OpenAI API returned an ServiceUnavailable Error: {e}\")\n",
    "\n",
    "            if tries < 10:\n",
    "                print(\n",
    "                    f\"Caught a ServiceUnavailable error: {e}. Waiting 10 seconds and then trying again...\"\n",
    "                )\n",
    "                failed = True\n",
    "                tries += 1\n",
    "                time.sleep(10)\n",
    "            else:\n",
    "                print(\n",
    "                    f\"Caught a ServiceUnavailable error: {e}. Too many exceptions. Giving up.\"\n",
    "                )\n",
    "                raise e\n",
    "\n",
    "        except openai.APIConnectionError as e:\n",
    "            print(f\"Failed to connect to OpenAI API: {e}\")\n",
    "            pass\n",
    "        except openai.RateLimitError as e:\n",
    "            print(f\"OpenAI API request exceeded rate limit: {e}\")\n",
    "            pass\n",
    "\n",
    "        # If the text is too long, we truncate it and try again. Note that if you get this error, you probably want to chunk your texts.\n",
    "        except openai.InvalidRequestError as e:\n",
    "            # Shorten request text\n",
    "            print(\n",
    "                f\"Received a InvalidRequestError. Request likely too long; cutting 10% of the text and trying again. {e}\"\n",
    "            )\n",
    "            time.sleep(5)\n",
    "            words = text.split()\n",
    "            num_words_to_remove = round(len(words) * 0.1)\n",
    "            remaining_words = words[:-num_words_to_remove]\n",
    "            text = \" \".join(remaining_words)\n",
    "            failed = True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Caught unhandled error.\")\n",
    "            pass\n",
    "\n",
    "    if response is None:\n",
    "        raise RuntimeError(\"API returned None (unexpected).\")\n",
    "    return \"\".join((c.message.content or \"\") for c in response.choices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5023524",
   "metadata": {},
   "source": [
    "### 5.2 Parse response "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7819cf84",
   "metadata": {},
   "source": [
    "The LLM will return a text message. We need to parse this response so that we can use it for further analysis. The details of this function will depend on how you asked the API to respond in your instruction (see above). In our case, we asked the LLM to return a list of numbers, followed by a motivation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e61620d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_result(result):\n",
    "    # The LLMs at times surround their answers with quotation marks, even if you explicitly tell them not to. If so, we remove them her.\n",
    "    result = result.strip(\"'\\\"\")\n",
    "    try:\n",
    "        # We asked the LLM to start with a number, followed by a semi-colon, followed by the motivation. We assume this format in the response here.\n",
    "        return result.split(\n",
    "            \";\", 2\n",
    "        )  # Split by ';' and use first part as numeric answer, second part as motivation\n",
    "    except Exception as e:\n",
    "        # If we get an error, we here print the string that failed, to allow debugging.\n",
    "        print(result)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1f18a0",
   "metadata": {},
   "source": [
    "### 5.3 Run the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637b7ea1",
   "metadata": {},
   "source": [
    "This is the main loop of the code, where we call the LLM for each line in our data, and give it the instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68fd472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we need to prepare the data and store it in a file for persistency\n",
    "filename = \"data.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a578ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the columns where we will store the analyzed data\n",
    "df[\"answers\"] = [[] for _ in range(len(df))]\n",
    "df[\"motivations\"] = [[] for _ in range(len(df))]\n",
    "\n",
    "df.to_pickle(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad269a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop\n",
    "df = pd.read_pickle(filename)\n",
    "\n",
    "# If you want to limit the number of lines to analyze\n",
    "maximum_lines_to_analyze = 10\n",
    "i = 0\n",
    "\n",
    "while True:\n",
    "\n",
    "    # Find all unprocessed lines\n",
    "    # left = df.loc[df['result'].isna()]\n",
    "    left = df.loc[df[\"answers\"].map(len) == 0]\n",
    "\n",
    "    # No lines left? Then we're done\n",
    "    if len(left) == 0 or i >= maximum_lines_to_analyze:\n",
    "        print(\"All done!\")\n",
    "        break\n",
    "\n",
    "    # Take a random line\n",
    "    line = left.sample()\n",
    "    index = line.index.values[0]\n",
    "\n",
    "    print(f\"There are {len(left)} left to process. Processing: {index}\")\n",
    "\n",
    "    # Wait for a bit, to not overload the API\n",
    "    time.sleep(WAIT_TIME)\n",
    "\n",
    "    # Analyze the specific line, chunk by chunk\n",
    "    for chunk in line[\"text_chunks\"].values[0]:\n",
    "        result = analyze_message(chunk, instruction, model=MODEL)\n",
    "\n",
    "        # Parse the results, and put into dataframe\n",
    "        answer, motivation = parse_result(result)\n",
    "\n",
    "        df.loc[index, \"answers\"].append(answer)\n",
    "        df.loc[index, \"motivations\"].append(motivation)\n",
    "\n",
    "    i += 1\n",
    "\n",
    "    # Save the result to persistent file\n",
    "    df.to_pickle(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0962128",
   "metadata": {},
   "source": [
    "### Post-analysis calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddfb960",
   "metadata": {},
   "source": [
    "Following the LLM analysis, we may need to do some minor calculations or modifications of the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babe771b",
   "metadata": {},
   "source": [
    "For instance, we need to combine the values returned for the different chunks to a final complete values for the full text. This can be done in several ways, but the most straight-forward is to take the average values for each part. If the text only has one chunk, the result will be used without change. We will here leave the motivations as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa67e7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(filename)\n",
    "\n",
    "# Take the mean level of populism in the text\n",
    "\n",
    "\n",
    "def safe_mean(values):\n",
    "    nums = pd.to_numeric(pd.Series(values), errors=\"coerce\").dropna()\n",
    "    return float(nums.mean()) if not nums.empty else None\n",
    "\n",
    "\n",
    "df[\"answer\"] = df[\"answers\"].apply(safe_mean)\n",
    "\n",
    "# Sort so that the most populist speeches are at the top\n",
    "df = df.sort_values(by=\"answer\", ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f80e21a",
   "metadata": {},
   "source": [
    "### Example result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af7ad52",
   "metadata": {},
   "source": [
    "We can now look at some examples of the result from the analysis and the associated motivation. \n",
    "\n",
    "For instance, we here look at the rating of Donald Trump's inaguration speech:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13252640",
   "metadata": {},
   "outputs": [],
   "source": [
    "fico = df.loc[df.filename == \"Slovakia_Fico_International_2.txt\"]\n",
    "print(\n",
    "    f\"\"\"Rating: {fico.answer.values[0]}. Motivation: '{fico.motivations.values[0][0]}'\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc73e23",
   "metadata": {},
   "source": [
    "At face value, this motivation seems both reasonable and plausible. We will now turn to carry out a more in-depth validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260db765",
   "metadata": {},
   "source": [
    "# 6. Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba5fed8",
   "metadata": {},
   "source": [
    "Finally, we need to validate our results. Careful validation is essential to make sure that the models are measuring what we intend -- and that they do so without problematic biases. To validate our models, we can compare the outputs with established benchmarks, ground truth data, or expert evaluations to validate the effectiveness in achieving the desired analysis outcomes. Validation can furthermore help us fine-tune the model prompt to improve the results.\n",
    "\n",
    "A simple way of validating can be to output a random sample to an Excel file, and have human coders manually classifying the data to compare the results. To do so, the code below can be used:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01f6645",
   "metadata": {},
   "source": [
    "### 6.1 Acquire validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bd52d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Code to extract the data as excel for manual checking. This is included for illustration, however, we won't use this here.\n",
    "# sample_size = 100\n",
    "# sample = df.sample(sample_size).reset_index()\n",
    "# sample['manual_classification'] = None\n",
    "# sample[['index','text','manual_classification']].to_excel('manual_validation.xlsx')\n",
    "\n",
    "# # Now open the resulting file in Excel. Carry out manual classification and put result in the final column\n",
    "\n",
    "# manual_result = pd.read_excel('manual_validation_finished.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a65f3b",
   "metadata": {},
   "source": [
    "In the case of the populism example, however, the Global Populism Database already offers a large sample of manually classified datapoints that we can use for validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637b4ccb",
   "metadata": {},
   "source": [
    "We first need to make sure the data is in the right format for running simpledorff. Each line should be one coder response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909b52f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and clean the validation data.\n",
    "val = pd.read_csv(\"./global-populism-dataset/gpd_v2_20220427.csv\")\n",
    "val = val[val[\"merging_variable\"].notna()]\n",
    "val = val[\n",
    "    val[\"rubricgrade\"].notna()\n",
    "]  # The database contains some NaN values for the index; we remove these lines\n",
    "val = val[[\"merging_variable\", \"codernum\", \"rubricgrade\", \"averagerubric\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8901d943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We include only the lines that we've coded with the LLM\n",
    "included = set(df.loc[~df[\"answer\"].isna()].filename.values)\n",
    "val = val.loc[(val[\"merging_variable\"].isin(included)) & (val[\"codernum\"] <= 2)]\n",
    "\n",
    "# We compare our result with that of the average coder result\n",
    "val = val.drop_duplicates(subset=[\"merging_variable\"], keep=\"first\")[\n",
    "    [\"merging_variable\", \"averagerubric\"]\n",
    "].rename(columns={\"averagerubric\": \"answer\"})\n",
    "val[\"codernum\"] = \"human\"\n",
    "\n",
    "# Fit our coded data into the same format to allow processing\n",
    "df2 = (\n",
    "    df[[\"filename\", \"answer\", \"motivations\"]]\n",
    "    .dropna(subset=[\"answer\"])\n",
    "    .rename(columns={\"filename\": \"merging_variable\"})\n",
    ")\n",
    "df2[\"codernum\"] = \"llm\"\n",
    "\n",
    "# Combine the two datasets\n",
    "validation_data = pd.concat([val, df2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0986fdb5",
   "metadata": {},
   "source": [
    "### 6.2 Measure Krippendorf's Alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86aa09f",
   "metadata": {},
   "source": [
    "To compare our data against the validation data, we can use Krippendorf's Alpha (see how-to guide for details.) We here use the simpledorff library to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79d3d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install simpledorff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b66f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import simpledorff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669fad4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate inter-coder reliability\n",
    "# Note that this uses the interval metric. If your variable is categorical, you need to remove the metric_fn parameter.\n",
    "KA = simpledorff.calculate_krippendorffs_alpha_for_df(\n",
    "    test,\n",
    "    metric_fn=simpledorff.metrics.interval_metric,\n",
    "    experiment_col=\"merging_variable\",\n",
    "    annotator_col=\"codernum\",\n",
    "    class_col=\"answer\",\n",
    ")\n",
    "\n",
    "print(f\"The resulting Krippendorf's Alpha is is {KA}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd28f349",
   "metadata": {},
   "source": [
    "This is a relatively high value for a first iteration of prompt development for a challenging concept.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b637f802",
   "metadata": {},
   "source": [
    "### 6.3 Carry out iterative process of concept and prompt development "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de11fff",
   "metadata": {},
   "source": [
    "Having measured the disagreements between coders and LLM, we can now seek to try to understand the sources of the disagreement. This can be best thought of as a process of mutual learning through which we develop and operationalize a rigorous social scientific concept in the form of a prompt.\n",
    "\n",
    "We can here work with the coders, and comparing their notes to the motivations given by the LLM, focusing on the examples where the LLM and the human coders are (most) in disagreement. We may find that the prompt can be improved - or that our human coders were mistaken or biased. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9d3f73",
   "metadata": {},
   "source": [
    "In our case, we do not have access to the coders, and we will simply show the process through which this form of work can be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b817dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a dataframe that lists the level of disagreement between coders and LLM\n",
    "wrong = df2.merge(val, on=\"merging_variable\")\n",
    "wrong[\"diff\"] = abs(wrong[\"answer_x\"] - wrong[\"answer_y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ebab4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can save as CSV file to analyze results in Excel, or examine the results here.\n",
    "# display(wrong.sort_values(['diff']))\n",
    "wrong.sort_values([\"diff\"]).to_csv(\"disagreements.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb291bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of the cases where the LLM and human coders disagree the most is a speech by Berlusconi. The LLM does not think it is populist, but the human coders do.\n",
    "\n",
    "# The motivation given by the LLM is:\n",
    "wrong.loc[\n",
    "    wrong[\"merging_variable\"] == \"Italy_Berlusconi_Ribbon_2.txt\"\n",
    "].motivations.values[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0527eb",
   "metadata": {},
   "source": [
    "Here follows the text, translated to English. \n",
    "\n",
    "Do you agree with the LLM or the huamn coders? If the latter, how do you think the prompt should be modified to improve the results? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d2e32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"\"\"Dear friends,\\n\\nIt is not easy to find the words to describe my, our state of mind at this moment. We are gathered here in Onna to celebrate the Liberation Day, a celebration that is both an honor and a commitment.\\n\\nAn honor: to commemorate a terrible massacre that took place right here in June 1944 when the Nazis, in retaliation, killed 17 citizens of Onna and then blew up the house where the bodies of those innocent victims were found.\\n\\nA commitment: what should inspire us is not to forget what happened here and to remember the horrors of totalitarianism and the suppression of 'freedom'.\\n\\nRight here, in Abruzzo, the legendary Maiella Brigade was born and operated, decorated with the Gold Medal for Military Valor. In December '43, 15 young people founded what would become the Maiella Brigade, which grew to 1,500 strong.\\n\\nIt is no coincidence that on this special day, the soldiers of the Honor Guard standing before us belong to the 33rd Artillery Regiment, the Abruzzesi unit that in 1943 on Cephalonia had the courage to resist the Nazis and sacrifice themselves – fighting – for the honor of our country.\\n\\nTo those patriots who fought for the redemption and rebirth of Italy, our admiration, gratitude, and recognition must always go.\\n\\nMost Italians today have not experienced what it means to be deprived of freedom. Only the elderly have a direct memory of totalitarianism, foreign occupation, and the war for the liberation of our homeland.\\n\\nFor many of us, it is a memory tied to our families, our parents, our grandparents, many of whom were protagonists or victims of those dramatic days. For me, it is the memory of years of separation from my father, forced to emigrate to avoid arrest, the memory of my mother's sacrifices, who alone had to support a large family during those difficult years. It is the memory of her courage, of her, like many others, traveling by train every day from a small town in the province of Como to work in Milan, and on one of those trains, risking her life but managing to save a Jewish woman from the clutches of a Nazi soldier destined for the extermination camps.\\n\\nThese are the memories, the examples with which we grew up – the memories of a generation of Italians who did not hesitate to choose freedom, even at the risk of their own safety and lives.\\n\\nOur country owes an inexhaustible debt to those many young people who sacrificed their lives during their most beautiful years to redeem the honor of the nation, out of fidelity to an oath, but above all for that great, splendid, and essential value which is freedom.\\n\\nWe owe the same debt of gratitude to all those other boys, Americans, English, French, Polish, from the many allied countries, who shed their blood in the Italian campaign. Without them, the sacrifice of our partisans would have risked being in vain.\\n\\nAnd with respect, we must remember today all the fallen, even those who fought on the wrong side, sincerely sacrificing their lives for their ideals and a lost cause.\\n\\nThis does not mean, of course, neutrality or indifference. We are – all free Italians are – on the side of those who fought for our freedom, for our dignity, and for the honor of our homeland.\\n\\nIn recent years, the history of the Resistance has been deepened and discussed. It is a good thing that it happened. The Resistance, along with the Risorgimento, is one of the founding values of our nation, a return to the tradition of freedom. And freedom is a right that comes before laws and the state because it is a natural right that belongs to us as human beings.\\n\\nHowever, a free nation does not need myths. As with the Risorgimento, we must also remember the dark pages of the civil war, even those in which those who fought on the right side made mistakes and took on blame.\\n\\nIt is an exercise in truth, in honesty, an exercise that makes the history of those who fought on the right side with selflessness and courage even more glorious.\\n\\nIt is the history of the many who fought in the Southern army, who, from Cephalonia onwards, redeemed the honor of the uniform with their blood.\\n\\nIt is the history of martyrs like Salvo D’Acquisto, who did not hesitate to sacrifice his life in exchange for other innocent lives.\\n\\nIt is the history of our soldiers interned in Germany who chose concentration camps rather than collaborating with the Nazis.\\n\\nIt is the history of the many who hid their fellow Jewish citizens, saving them from deportation.\\n\\nAbove all, it is the history of the many, countless unknown heroes who, with small or great acts of daily courage, contributed to the cause of freedom.\\n\\nEven the Church, I want to remember, played its part with true courage, to prevent odious concepts like race or religious differences from becoming reasons for persecution and death.\\n\\nSimilarly, we must remember the young Jews of the Jewish Brigade, who came from ghettos all over Europe, took up arms, and fought for freedom.\\n\\nAt that moment, many Italians of different faiths, cultures, and backgrounds came together to pursue the same great dream – the dream of freedom.\\n\\nAmong them were very different individuals and groups. Some thought only of freedom, some dreamed of establishing a different social and political order, some considered themselves bound by an oath of loyalty to the monarchy.\\n\\nBut they all managed to set aside their differences, even the most profound ones, to fight together. The communists and the Catholics, the socialists and the liberals, the actionists and the monarchists, faced with a common tragedy, each wrote a great page of our history. A page on which our Constitution is based, a page on which our freedom is based.\\n\\nIn the drafting of the Constitution, the wisdom of the political leaders of that time – De Gasperi and Togliatti, Ruini and Terracini, Nenni, Pacciardi, and Parri – managed to channel deep initial divisions towards a single objective.\\n\\nAlthough clearly the result of compromises, the republican Constitution achieved two noble and fundamental objectives: guaranteeing freedom and creating the conditions for democratic development in the country. It was not a small feat; in fact, it was the best compromise possible at the time.\\n\\nHowever, the goal of creating a \"common\" moral conscience for the nation was missed, perhaps premature for those times, so much so that the predominant value for everyone was anti-fascism, but not necessarily anti-totalitarianism. It was a product of history, a compromise useful to avoid the Cold War that vertically divided Italy from degenerating into a civil war with unpredictable outcomes. But the assumption of responsibility and the sense of the State that animated all the political leaders of that time remain a great lesson that would be unforgivable to forget.\\n\\nToday, 64 years after April 25, 1945, and twenty years after the fall of the Berlin Wall, our task, the task of all, is to finally build a unified national sentiment.\\n\\nWe must do it together, together, regardless of political affiliation, together, for a new beginning of our republican democracy, where all political parties recognize the greatest value, freedom, and debate in its name for the good and the interest of all.\\n\\nThe anniversary of the regained freedom is, therefore, an opportunity to reflect on the past, but also to reflect on the present and the future of Italy. If we can do it together from today onwards, we will have rendered a great service not to one\\n\\nWe have always rejected the idea that our adversary was our enemy. Our religion of freedom demanded it from us and still does. With the same spirit, I am convinced that the time has come for the Liberation Day to become the Day of Freedom, and for this commemoration to shed the character of opposition that revolutionary culture gave it, a character that still 'divides' rather than 'unites'.\\n\\nI say this with great serenity, without any intention of creating controversy. April 25 was the origin of a new season of democracy, and in democracy, the people's vote deserves absolute respect from everyone.\\n\\nAfter April 25, the people peacefully voted for the Republic, and the monarchy accepted the people's judgment.\\n\\nShortly after, on April 18, 1948, the people's choice was once again decisive for our country: with De Gasperi's victory, the Italian people recognized themselves in the Christian and liberal tradition of their history. The 1950s, always with the support of the popular vote, shaped Italy into a democratic, economic, and social reality. Italy became part of Europe and the West, played a role in promoting Atlantic unity and European unity, transforming from a rejected nation to a respected one.\\n\\nToday, our young people face other challenges: to defend the freedom conquered by their fathers and expand it even further, aware that without freedom, there can be no peace, justice, or well-being.\\n\\nSome of these challenges are global and see us engaged alongside free nations: the fight against terrorism, the fight against fanatic and repressive fundamentalism, the fight against racism because freedom, dignity, and peace are rights of every human being, 'everywhere' in the world.\\n\\nThat's why I want to remember the Italian soldiers engaged in peace missions abroad, especially those who have fallen in carrying out this noble mission. There is an ideal continuity between them and all the heroes, Italian and allied, who sacrificed their lives over 60 years ago to give us back freedom, security, and peace.\\n\\nToday, the teachings of our fathers take on a special value: this April 25 comes just after the great tragedy that struck this land of Abruzzo. Once again, facing the emergency and tragedy, Italians have shown their ability to unite, to overcome differences, demonstrating that they are a great and cohesive people, full of generosity, solidarity, and courage.\\n\\nLooking at the many Italians who have been engaged here in rescue and reconstruction efforts, I feel proud, once again, even more so, to be Italian and to lead this wonderful country.\\n\\nToday, Onna is the symbol of our Italy. The earthquake that destroyed it reminds us of the days when invaders destroyed it. Rebuilding it will mean repeating the gesture of its rebirth after Nazi violence.\\n\\nAnd it is precisely concerning the heroes of then and today that we all have a great responsibility: to set aside any controversy, to look at the interest of the nation, to safeguard the great heritage of freedom that we inherited from our fathers.\\n\\nTogether, we all have the responsibility and duty to build a future of prosperity, security, peace, and freedom for all.\\n\\nLong live Italy! Long live the Republic!\\n\\nLong live April 25, the celebration of all Italians who love freedom and want to remain free!\\n\\nLong live April 25, the celebration of regained freedom!\"\"\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
